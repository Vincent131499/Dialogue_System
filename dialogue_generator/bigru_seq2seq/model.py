import tensorflow as tf


class Seq2SeqGru(object):
    def __init__(self, config, vocab_size, word_vectors=None):
        self.vocab_size = vocab_size
        self.word_vectors = word_vectors  # 词向量得到的词嵌入矩阵

        self.batch_size = config["batch_size"]
        self.embedding_size = config["embedding_size"]
        self.encoder_hidden_sizes = config["encoder_hidden_sizes"]
        self.decoder_hidden_sizes = config["decoder_hidden_sizes"]
        self.dropout_rate = config["dropout_rate"]
        self.learning_rate = config["learning_rate"]  # 学习速率
        self.smooth_rate = config["smooth_rate"]  # smooth label的比例
        self.warmup_step = config["warmup_step"]  # 学习速率预热的步数
        self.decode_step = config["decode_step"]  # 解码的最大长度
        self.max_grad_norm = config["max_grad_norm"]

        self.pad_token = 0

        # 编码和解码共享embedding矩阵，若是不同语言的，如机器翻译，就各定义一个embedding矩阵
        self.embedding_matrix = self._get_embedding_matrix()
        self.saver = self.init_saver()
        self.optimizer = self.init_optimizer()

    def _get_embedding_matrix(self, zero_pad=True):
        """
        词嵌入层
        :param zero_pad:
        :return:
        """
        with tf.variable_scope("embedding"):
            embeddings = tf.get_variable('embedding_w',
                                         dtype=tf.float32,
                                         shape=(self.vocab_size, self.embedding_size),
                                         initializer=tf.contrib.layers.xavier_initializer())
            if zero_pad:
                embeddings = tf.concat((tf.zeros(shape=[1, self.embedding_size]),
                                        embeddings[1:, :]), 0)

        return embeddings

    def encoder(self, encoder_inputs, encoder_length, encoder_max_len, training=True):
        """
        定义encoder层
        :param encoder_inputs:
        :param encoder_length:
        :param encoder_max_len:
        :param training:
        :return:
        """
        with tf.name_scope("encoder"):
            # 词嵌入层，并加上位置向量
            embedded_word = tf.nn.embedding_lookup(self.embedding_matrix, encoder_inputs)
            embedded_word = tf.layers.dropout(embedded_word, rate=self.dropout_rate, training=training)
            # 得到encoder mask矩阵
            encoder_mask = tf.sequence_mask(encoder_length, encoder_max_len, dtype=tf.float32, name="encoder_mask")
            input_size = self.embedding_size
            for idx, hidden_size in enumerate(self.encoder_hidden_sizes):
                with tf.name_scope("gru_{}".format(idx)):
                    initial_state = tf.zeros([self.batch_size, hidden_size], dtype=tf.float32, name="initial_state")
                    embedded_word, state = self.encoder_layer(input_size, hidden_size,
                                                              initial_state, embedded_word, encoder_mask)
                input_size = hidden_size

        return embedded_word, encoder_mask, state

    @staticmethod
    def encoder_layer(input_size, hidden_size, initial_state, inputs, input_mask):
        """
        单向gru结构层
        :param input_size: 输入的大小
        :param hidden_size: 隐层的大小
        :param initial_state: 初始状态
        :param inputs: 输入
        :param input_mask: 输入的mask
        :return: 输出，最终的状态
        """
        # 创建一个gru cell对象
        cell = GRUCell(input_size, hidden_size)
        outputs = []
        state = initial_state
        """
        按照序列从前到后遍历，将每一时间的输出添加到outputs中，tf.unstack是解包函数，可以按照序列维度解包成一个个的
        [batch_size, input_size]的tensor，input_mask是为了对padding部分做mask处理
        """
        for time, (embedded, mask) in enumerate(zip(tf.unstack(tf.transpose(inputs, [1, 0, 2])),
                                                    tf.unstack(tf.transpose(input_mask, [1, 0])))):
            output, state = cell(embedded, state)
            output = tf.expand_dims(mask, 1) * output
            outputs.append(output)
        outputs = tf.transpose(tf.convert_to_tensor(outputs, dtype=tf.float32), [1, 0, 2])
        return outputs, state

    @staticmethod
    def bidirectional_encoder_layer(input_size, hidden_size, initial_fw_state, initial_bw_state, fw_inputs, bw_inputs,
                                    fw_input_mask, bw_input_mask):
        """
        双向gru结构层
        :param input_size: 输入的大小
        :param hidden_size: 隐层的大小
        :param initial_fw_state: 前向的初始状态
        :param initial_bw_state: 后向的初始状态
        :param fw_inputs: 前向输入
        :param bw_inputs: 反向输入
        :param fw_input_mask: 前向输入的mask
        :param bw_input_mask: 反响输入的mask
        :return:
        """
        cell = GRUCell(input_size, hidden_size)
        fw_outputs = []
        bw_outputs = []
        fw_state = initial_fw_state
        bw_state = initial_bw_state
        for fw_time, (fw_embedded, fw_mask) in enumerate(zip(tf.unstack(tf.transpose(fw_inputs, [1, 0, 2])),
                                                             tf.unstack(tf.transpose(fw_input_mask, [1, 0])))):
            fw_output, fw_state = cell(fw_embedded, fw_state)
            fw_output = tf.expand_dims(fw_mask, 1) * fw_output
            fw_outputs.append(fw_output)

        for bw_time, (bw_embedded, bw_mask) in enumerate(zip(tf.unstack(tf.transpose(bw_inputs, [1, 0, 2])),
                                                             tf.unstack(tf.transpose(bw_input_mask, [1, 0])))):
            bw_output, bw_state = cell(bw_embedded, bw_state)
            bw_output = tf.expand_dims(bw_mask, 1) * bw_output
            bw_outputs.append(bw_output)

        fw_outputs = tf.transpose(tf.convert_to_tensor(fw_outputs, dtype=tf.float32), [1, 0, 2])
        bw_outputs = tf.transpose(tf.convert_to_tensor(bw_outputs, dtype=tf.float32), [1, 0, 2])

        return (fw_outputs, bw_outputs), (fw_state, bw_state)

    def decoder(self, encoder_outputs, encoder_mask, decoder_inputs, decoder_length, decoder_max_len,
                encoder_final_state=None, encoder_length=None, use_attention=True, training=True):
        """
        定义decoder
        :param encoder_outputs:
        :param encoder_mask:
        :param decoder_inputs:
        :param decoder_length:
        :param decoder_max_len:
        :param encoder_final_state:
        :param encoder_length:
        :param use_attention:
        :param training:
        :return:
        """
        with tf.name_scope("decoder"):
            # 词嵌入层，并加上位置向量
            embedded_word = tf.nn.embedding_lookup(self.embedding_matrix, decoder_inputs)
            embedded_word = tf.layers.dropout(embedded_word, rate=self.dropout_rate, training=training)
            # 得到encoder mask矩阵
            decoder_mask = tf.sequence_mask(decoder_length, decoder_max_len, dtype=tf.float32, name="encoder_mask")
            input_size = self.embedding_size
            for idx, hidden_size in enumerate(self.decoder_hidden_sizes):
                with tf.name_scope("gru_{}".format(idx)):
                    if encoder_final_state is not None:
                        initial_state = encoder_final_state
                    else:
                        initial_state = tf.zeros([self.batch_size, hidden_size], dtype=tf.float32, name="initial_state")
                    embedded_word, state = self.decoder_layer(input_size, hidden_size, initial_state, encoder_outputs,
                                                              encoder_mask, embedded_word, decoder_mask,
                                                              encoder_length, use_attention)
                input_size = hidden_size

        weights = tf.transpose(self.embedding_matrix)  # (d_model, vocab_size)
        logits = tf.einsum('ntd,dk->ntk', embedded_word, weights)  # (N, T2, vocab_size)

        return logits

    def decoder_layer(self, input_size, hidden_size, initial_state, encoder_outputs, encoder_mask,
                      decoder_inputs, decoder_mask, encoder_length, use_attention=True):
        """
        定义decoder层
        :param input_size: 输入大小
        :param hidden_size: 隐层大小
        :param initial_state: 初始状态
        :param encoder_outputs: encoder的输出
        :param encoder_mask: encoder的mask
        :param decoder_inputs: decoder的输入
        :param decoder_mask: decoder的mask
        :param encoder_length: encoder的实际长度
        :param use_attention: 是否用attention
        :return: 输出和结束状态
        """

        if not use_attention and not encoder_length:
            raise ("if not use attention, please be sure encoder_length is not None")

        # 创建一个gru cell对象
        cell = GRUCell(input_size, hidden_size, use_attention=use_attention)
        outputs = []
        state = initial_state

        # 因为encoder有部分是padding的，因此再取最后时间步的输出作为整个encoder的编码时，
        # 我们需要取非padding部分的最后时间步的输出
        col = tf.convert_to_tensor(encoder_length, dtype=tf.int32) - 1
        row = tf.range(self.batch_size)
        index = tf.unstack(tf.stack([row, col], axis=0), axis=1)
        encoder_final_output = tf.gather_nd(encoder_outputs, index)
        for time, (embedded, mask) in enumerate(zip(tf.unstack(tf.transpose(decoder_inputs, [1, 0, 2])),
                                                    tf.unstack(tf.transpose(decoder_mask, [1, 0])))):
            if not use_attention:
                c = encoder_final_output
            else:
                c = self._attention(embedded, encoder_outputs, encoder_mask)
            output, state = cell(embedded, state, c)
            output = tf.expand_dims(mask, 1) * output
            outputs.append(output)
        outputs = tf.transpose(tf.convert_to_tensor(outputs, dtype=tf.float32), [1, 0, 2])
        return outputs, state

    @staticmethod
    def _attention(embedded, encoder_outputs, encoder_mask):
        """
        encoder decoder之间的attention机制
        :param embedded:
        :param encoder_outputs:
        :param encoder_mask:
        :return:
        """
        embedded = tf.expand_dims(embedded, -1)
        similarity = tf.matmul(encoder_outputs, embedded)
        weight = tf.nn.softmax(similarity, axis=-1)
        weight *= tf.expand_dims(encoder_mask, -1)
        # 注，在这里的weight必须放在前面，表示对encoder_outputs中做行相加，即沿着序列做加权和
        c = tf.squeeze(tf.matmul(tf.transpose(weight, [0, 2, 1]), encoder_outputs))

        return c

    def label_smoothing(self, inputs):
        """
        标签平滑，将原本的one-hot真实标签向量变成一个不含0的标签向量
        :param inputs:
        :return:
        """
        V = inputs.get_shape().as_list()[-1]
        return ((1 - self.smooth_rate) * inputs) + (self.smooth_rate / V)

    @staticmethod
    def noam_scheme(init_lr, global_step, warmup_steps=4000.):
        """
        采用预热学习速率的方法来训练模型
        :param init_lr:
        :param global_step:
        :param warmup_steps:
        :return:
        """
        step = tf.cast(global_step + 1, dtype=tf.float32)
        return init_lr * warmup_steps ** 0.5 * tf.minimum(step * warmup_steps ** -1.5, step ** -0.5)

    @staticmethod
    def init_saver():
        """
        初始化saver对象
        :return:
        """
        saver = tf.train.Saver(tf.global_variables(), max_to_keep=100)
        return saver

    def init_optimizer(self):
        optimizer = tf.train.AdamOptimizer(self.learning_rate)
        return optimizer

    def train(self, sess, batch):
        """
        对于训练阶段，需要执行self.train_op, self.loss, self.summary_op三个op，并传入相应的数据
        :param sess:
        :param batch:
        :return:
        """
        encoder_outputs, encoder_mask, final_state = self.encoder(encoder_inputs=batch["encoder_inputs"],
                                                                  encoder_length=batch["encoder_length"],
                                                                  encoder_max_len=max(batch["encoder_length"]))

        logits = self.decoder(encoder_outputs=encoder_outputs,
                              encoder_mask=encoder_mask,
                              decoder_inputs=batch["decoder_inputs"],
                              decoder_length=batch["decoder_length"],
                              decoder_max_len=max(batch["decoder_length"]),
                              encoder_final_state=final_state,
                              encoder_length=batch["encoder_length"],
                              use_attention=True)

        predictions = tf.to_int32(tf.argmax(logits, axis=-1))
        # train scheme
        # 对真实的标签做平滑处理
        y_ = self.label_smoothing(tf.one_hot(batch["decoder_outputs"], depth=self.vocab_size))

        losses = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=y_)
        # 取出非0部分，即非padding部分
        non_padding = tf.to_float(tf.not_equal(batch["decoder_outputs"], self.pad_token))
        loss = tf.reduce_sum(losses * non_padding) / (tf.reduce_sum(non_padding) + 1e-7)
        trainable_params = tf.trainable_variables()
        gradients = tf.gradients(loss, trainable_params)
        # 对梯度进行梯度截断
        clip_gradients, _ = tf.clip_by_global_norm(gradients, self.max_grad_norm)
        train_op = self.optimizer.apply_gradients(zip(clip_gradients, trainable_params))

        tf.summary.scalar("loss", loss)
        summary_op = tf.summary.merge_all()

        # 训练模型
        _, summary, loss, predictions = sess.run([train_op, summary_op, loss, predictions])
        return summary, loss, predictions

    def eval(self, sess, batch):
        """
        对于eval阶段，不需要反向传播，所以只执行self.loss, self.summary_op两个op，并传入相应的数据
        :param sess:
        :param batch:
        :return:
        """
        encoder_outputs, encoder_mask, final_state = self.encoder(encoder_inputs=batch["encoder_inputs"],
                                                                  encoder_length=batch["encoder_length"],
                                                                  encoder_max_len=max(batch["encoder_length"]))

        logits = self.decoder(encoder_outputs=encoder_outputs,
                              encoder_mask=encoder_mask,
                              decoder_inputs=batch["decoder_inputs"],
                              decoder_length=batch["decoder_length"],
                              decoder_max_len=batch["decoder_max_len"],
                              encoder_final_state=final_state,
                              encoder_length=batch["encoder_length"],
                              use_attention=True)

        predictions = tf.to_int32(tf.argmax(logits, axis=-1))
        # train scheme
        # 对真实的标签做平滑处理
        y_ = self.label_smoothing(tf.one_hot(batch["decoder_outputs"], depth=self.vocab_size))
        losses = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=y_)
        # 取出非0部分，即非padding部分
        non_padding = tf.to_float(tf.not_equal(batch["decoder_outputs"], self.pad_token))
        loss = tf.reduce_sum(losses * non_padding) / (tf.reduce_sum(non_padding) + 1e-7)

        tf.summary.scalar("loss", loss)
        summary_op = tf.summary.merge_all()

        # 训练模型
        summary, loss, predictions = sess.run([summary_op, loss, predictions])
        return summary, loss, predictions


class GRUCell(object):
    """
    GRU神经单元实现
    """

    def __init__(self, input_size, hidden_size, use_attention=False, activation=tf.nn.tanh):
        # Initialize parameters
        self._activation = activation  # 激活函数
        self._input_size = input_size  # 输入的大小
        self._hidden_size = hidden_size  # 隐层的大小
        self._use_attention = use_attention  # 是否选择attention，decoder的时候设为True
        # 定义更新门的权重系数
        self._W_r_x = tf.Variable(
            tf.random_uniform([self._input_size, self._hidden_size], -0.1, 0.1, tf.float32), name="w_r_x")
        self._W_r_h = tf.Variable(
            tf.random_uniform([self._hidden_size, self._hidden_size], -0.1, 0.1, tf.float32), name="w_r_h")
        # 定义重置门的权重系数
        self._W_z_x = tf.Variable(
            tf.random_uniform([self._input_size, self._hidden_size], -0.1, 0.1, tf.float32), name="w_z_x")
        self._W_z_h = tf.Variable(
            tf.random_uniform([self._hidden_size, self._hidden_size], -0.1, 0.1, tf.float32), name="w_z_h")
        # 定义w_h的权重系数
        self._W_h_x = tf.Variable(
            tf.random_uniform([self._input_size, self._hidden_size], -0.1, 0.1, tf.float32), name="w_h_x")
        self._W_h_h = tf.Variable(
            tf.random_uniform([self._hidden_size, self._hidden_size], -0.1, 0.1, tf.float32), name="w_h_h")

        # 定义更新门的偏置
        self._B_r = tf.Variable(tf.ones([self._hidden_size], tf.float32), name="b_r")
        # 定义重置门的偏置
        self._B_z = tf.Variable(tf.ones([self._hidden_size], tf.float32), name="b_z")
        # 定义b_h的偏置
        self._B_h = tf.Variable(tf.ones([self._hidden_size], tf.float32), name="b_h")

        if self._use_attention:
            # 更新门中对attention的结果的权重系数
            self._W_r_c = tf.Variable(
                tf.random_uniform([self._hidden_size, self._hidden_size], -0.1, 0.1, tf.float32), name="w_r_c")
            # 重置门中对attention的结果的权重系数
            self._W_z_c = tf.Variable(
                tf.random_uniform([self._hidden_size, self._hidden_size], -0.1, 0.1, tf.float32), name="w_z_c")
            # h中对attention的结果的权重系数
            self._W_h_c = tf.Variable(
                tf.random_uniform([self._hidden_size, self._hidden_size], -0.1, 0.1, tf.float32), name="w_h_c")

    def __call__(self, inputs, state, context=None):
        """
        传入输入，隐层状态，attention的结果，输出下一状态
        :param inputs:
        :param state:
        :param context:
        :return:
        """
        if not self._use_attention:
            # 得到更新门
            r = tf.sigmoid(tf.matmul(tf.concat([inputs, state], axis=1),
                                     tf.concat([self._W_r_x, self._W_r_h], axis=0)) + self._B_r, name="r")
            # 得到重置门
            z = tf.sigmoid(tf.matmul(tf.concat([inputs, state], axis=1),
                                     tf.concat([self._W_z_x, self._W_z_h], axis=0)) + self._B_z, name="z")
        else:
            if context is None:
                raise ValueError("Attention mechanism used, while context vector is not received.")
            # 得到更新门

            r = tf.sigmoid(tf.matmul(tf.concat([inputs, state, context], axis=1),
                                     tf.concat([self._W_r_x, self._W_r_h, self._W_r_c], axis=0)) + self._B_r, name="r")
            # 得到重置门
            z = tf.sigmoid(tf.matmul(tf.concat([inputs, state, context], axis=1),
                                     tf.concat([self._W_z_x, self._W_z_h, self._W_z_c], axis=0)) + self._B_z, name="z")

        if not self._use_attention:
            h_hat = self._activation(tf.matmul(tf.concat([inputs, r * state], axis=1),
                                               tf.concat([self._W_h_x, self._W_h_h], axis=0)) + self._B_h, name="h_hat")
        else:
            h_hat = self._activation(
                tf.matmul(tf.concat([inputs, r * state, context], axis=1),
                          tf.concat([self._W_h_x, self._W_h_h, self._W_h_c], axis=0)) + self._B_h, name="h_hat")
        h = z * state + (1 - z) * h_hat
        return h, h
